{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EfficientUnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## layer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class Conv2dSamePadding(nn.Conv2d):\n",
    "    \"\"\"2D Convolutions with same padding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, groups=1, bias=True, name=None):\n",
    "        super().__init__(in_channels, out_channels, kernel_size, stride, padding=0, dilation=dilation, groups=groups,\n",
    "                         bias=bias)\n",
    "        self.stride = self.stride if len(self.stride) == 2 else [self.stride[0]] * 2\n",
    "        self.name = name\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_h, input_w = x.size()[2:]\n",
    "        kernel_h, kernel_w = self.weight.size()[2:]\n",
    "        stride_h, stride_w = self.stride\n",
    "        output_h, output_w = math.ceil(input_h / stride_h), math.ceil(input_w / stride_w)\n",
    "        pad_h = max((output_h - 1) * self.stride[0] + (kernel_h - 1) * self.dilation[0] + 1 - input_h, 0)\n",
    "        pad_w = max((output_w - 1) * self.stride[1] + (kernel_w - 1) * self.dilation[1] + 1 - input_w, 0)\n",
    "        if pad_h > 0 or pad_w > 0:\n",
    "            x = F.pad(x, [pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2])\n",
    "        return F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "\n",
    "class BatchNorm2d(nn.BatchNorm2d):\n",
    "    def __init__(self, num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, name=None):\n",
    "        super().__init__(num_features, eps=eps, momentum=momentum, affine=affine,\n",
    "                         track_running_stats=track_running_stats)\n",
    "        self.name = name\n",
    "\n",
    "\n",
    "def drop_connect(inputs, drop_connect_rate, training):\n",
    "    if not training:\n",
    "        return inputs\n",
    "    batch_size = inputs.shape[0]\n",
    "    keep_prob = 1.0 - drop_connect_rate\n",
    "    random_tensor = keep_prob\n",
    "    random_tensor += torch.rand([batch_size, 1, 1, 1], dtype=inputs.dtype, device=inputs.device)\n",
    "    binary_tensor = torch.floor(random_tensor)\n",
    "    output = inputs / keep_prob * binary_tensor\n",
    "    return output\n",
    "\n",
    "\n",
    "class MBConvBlock(nn.Module):\n",
    "    \"\"\"Mobile Inverted Residual Bottleneck Block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, block_args, global_params, idx):\n",
    "        super().__init__()\n",
    "\n",
    "        block_name = 'blocks_' + str(idx) + '_'\n",
    "\n",
    "        self.block_args = block_args\n",
    "        self.batch_norm_momentum = 1 - global_params.batch_norm_momentum\n",
    "        self.batch_norm_epsilon = global_params.batch_norm_epsilon\n",
    "        self.has_se = (self.block_args.se_ratio is not None) and (0 < self.block_args.se_ratio <= 1)\n",
    "        self.id_skip = block_args.id_skip\n",
    "\n",
    "        self.swish = Swish(block_name + '_swish')\n",
    "\n",
    "        # Expansion phase\n",
    "        in_channels = self.block_args.input_filters\n",
    "        out_channels = self.block_args.input_filters * self.block_args.expand_ratio\n",
    "        if self.block_args.expand_ratio != 1:\n",
    "            self._expand_conv = Conv2dSamePadding(in_channels=in_channels,\n",
    "                                                  out_channels=out_channels,\n",
    "                                                  kernel_size=1,\n",
    "                                                  bias=False,\n",
    "                                                  name=block_name + 'expansion_conv')\n",
    "            self._bn0 = BatchNorm2d(num_features=out_channels,\n",
    "                                    momentum=self.batch_norm_momentum,\n",
    "                                    eps=self.batch_norm_epsilon,\n",
    "                                    name=block_name + 'expansion_batch_norm')\n",
    "\n",
    "        # Depth-wise convolution phase\n",
    "        kernel_size = self.block_args.kernel_size\n",
    "        strides = self.block_args.strides\n",
    "        self._depthwise_conv = Conv2dSamePadding(in_channels=out_channels,\n",
    "                                                 out_channels=out_channels,\n",
    "                                                 groups=out_channels,\n",
    "                                                 kernel_size=kernel_size,\n",
    "                                                 stride=strides,\n",
    "                                                 bias=False,\n",
    "                                                 name=block_name + 'depthwise_conv')\n",
    "        self._bn1 = BatchNorm2d(num_features=out_channels,\n",
    "                                momentum=self.batch_norm_momentum,\n",
    "                                eps=self.batch_norm_epsilon,\n",
    "                                name=block_name + 'depthwise_batch_norm')\n",
    "\n",
    "        # Squeeze and Excitation layer\n",
    "        if self.has_se:\n",
    "            num_squeezed_channels = max(1, int(self.block_args.input_filters * self.block_args.se_ratio))\n",
    "            self._se_reduce = Conv2dSamePadding(in_channels=out_channels,\n",
    "                                                out_channels=num_squeezed_channels,\n",
    "                                                kernel_size=1,\n",
    "                                                name=block_name + 'se_reduce')\n",
    "            self._se_expand = Conv2dSamePadding(in_channels=num_squeezed_channels,\n",
    "                                                out_channels=out_channels,\n",
    "                                                kernel_size=1,\n",
    "                                                name=block_name + 'se_expand')\n",
    "\n",
    "        # Output phase\n",
    "        final_output_channels = self.block_args.output_filters\n",
    "        self._project_conv = Conv2dSamePadding(in_channels=out_channels,\n",
    "                                               out_channels=final_output_channels,\n",
    "                                               kernel_size=1,\n",
    "                                               bias=False,\n",
    "                                               name=block_name + 'output_conv')\n",
    "        self._bn2 = BatchNorm2d(num_features=final_output_channels,\n",
    "                                momentum=self.batch_norm_momentum,\n",
    "                                eps=self.batch_norm_epsilon,\n",
    "                                name=block_name + 'output_batch_norm')\n",
    "\n",
    "    def forward(self, x, drop_connect_rate=None):\n",
    "        identity = x\n",
    "        # Expansion and depth-wise convolution\n",
    "        if self.block_args.expand_ratio != 1:\n",
    "            x = self._expand_conv(x)\n",
    "            x = self._bn0(x)\n",
    "            x = self.swish(x)\n",
    "\n",
    "        x = self._depthwise_conv(x)\n",
    "        x = self._bn1(x)\n",
    "        x = self.swish(x)\n",
    "\n",
    "        # Squeeze and Excitation\n",
    "        if self.has_se:\n",
    "            x_squeezed = F.adaptive_avg_pool2d(x, 1)\n",
    "            x_squeezed = self._se_expand(self.swish(self._se_reduce(x_squeezed)))\n",
    "            x = torch.sigmoid(x_squeezed) * x\n",
    "\n",
    "        x = self._bn2(self._project_conv(x))\n",
    "\n",
    "        # Skip connection and drop connect\n",
    "        input_filters, output_filters = self.block_args.input_filters, self.block_args.output_filters\n",
    "        if self.id_skip and self.block_args.strides == 1 and input_filters == output_filters:\n",
    "            if drop_connect_rate:\n",
    "                x = drop_connect(x, drop_connect_rate=drop_connect_rate, training=self.training)\n",
    "            x = x + identity\n",
    "        return x\n",
    "\n",
    "\n",
    "def double_conv(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "def up_conv(in_channels, out_channels):\n",
    "    return nn.ConvTranspose2d(\n",
    "        in_channels, out_channels, kernel_size=2, stride=2\n",
    "    )\n",
    "\n",
    "\n",
    "def custom_head(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Dropout(),\n",
    "        nn.Linear(in_channels, 512),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(),\n",
    "        nn.Linear(512, out_channels)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "GlobalParams = namedtuple('GlobalParams', ['batch_norm_momentum', 'batch_norm_epsilon', 'dropout_rate', 'num_classes',\n",
    "                                           'width_coefficient', 'depth_coefficient', 'depth_divisor', 'min_depth',\n",
    "                                           'drop_connect_rate'])\n",
    "GlobalParams.__new__.__defaults__ = (None,) * len(GlobalParams._fields)\n",
    "\n",
    "BlockArgs = namedtuple('BlockArgs', ['kernel_size', 'num_repeat', 'input_filters', 'output_filters', 'expand_ratio',\n",
    "                                     'id_skip', 'strides', 'se_ratio'])\n",
    "BlockArgs.__new__.__defaults__ = (None,) * len(BlockArgs._fields)\n",
    "\n",
    "\n",
    "IMAGENET_WEIGHTS = {\n",
    "    'efficientnet-b0': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b0-355c32eb.pth',\n",
    "    'efficientnet-b1': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b1-f1951068.pth',\n",
    "    'efficientnet-b2': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b2-8bb594d6.pth',\n",
    "    'efficientnet-b3': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b3-5fb5a3c3.pth',\n",
    "    'efficientnet-b4': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b4-6ed6700e.pth',\n",
    "    'efficientnet-b5': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b5-b6417697.pth',\n",
    "    'efficientnet-b6': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b6-c76e70fd.pth',\n",
    "    'efficientnet-b7': 'https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b7-dcc49843.pth',\n",
    "\n",
    "    'efficientnet-b0-socar' : '/home/pung/repo/kimin-lab/models/pretrained_effb0_orignal_socar_50000.pt',\n",
    "    'efficientnet-b1-socar' : '/home/pung/repo/kimin-lab/models/pretrained_effb1_orignal_socar_25.pt',\n",
    "    'efficientnet-b4-socar' : '/home/pung/repo/kimin-lab/models/pretrained_effb4_orignal_socar_25.pt',\n",
    "\n",
    "    'efficientnet-b0-stanford' : '/home/pung/repo/kimin-lab/models/pretrained_effb0_orignal_stanford_50000.pt',\n",
    "    'efficientnet-b4-stanford' : '/home/pung/repo/kimin-lab/models/pretrained_effb4_orignal_stanford_25.pt',\n",
    "}\n",
    "\n",
    "\n",
    "def round_filters(filters, global_params):\n",
    "    \"\"\"Round number of filters\n",
    "    \"\"\"\n",
    "    multiplier = global_params.width_coefficient\n",
    "    divisor = global_params.depth_divisor\n",
    "    min_depth = global_params.min_depth\n",
    "    if not multiplier:\n",
    "        return filters\n",
    "\n",
    "    filters *= multiplier\n",
    "    min_depth = min_depth or divisor\n",
    "    new_filters = max(min_depth, int(filters + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_filters < 0.9 * filters:\n",
    "        new_filters += divisor\n",
    "    return int(new_filters)\n",
    "\n",
    "\n",
    "def round_repeats(repeats, global_params):\n",
    "    \"\"\"Round number of repeats\n",
    "    \"\"\"\n",
    "    multiplier = global_params.depth_coefficient\n",
    "    if not multiplier:\n",
    "        return repeats\n",
    "    return int(math.ceil(multiplier * repeats))\n",
    "\n",
    "\n",
    "def get_efficientnet_params(model_name, override_params=None):\n",
    "    \"\"\"Get efficientnet params based on model name\n",
    "    \"\"\"\n",
    "    model_name = model_name[:15]\n",
    "\n",
    "    params_dict = {\n",
    "        # (width_coefficient, depth_coefficient, resolution, dropout_rate)\n",
    "        # Note: the resolution here is just for reference, its values won't be used.\n",
    "        'efficientnet-b0': (1.0, 1.0, 224, 0.2),\n",
    "        'efficientnet-b1': (1.0, 1.1, 240, 0.2),\n",
    "        'efficientnet-b2': (1.1, 1.2, 260, 0.3),\n",
    "        'efficientnet-b3': (1.2, 1.4, 300, 0.3),\n",
    "        'efficientnet-b4': (1.4, 1.8, 380, 0.4),\n",
    "        'efficientnet-b5': (1.6, 2.2, 456, 0.4),\n",
    "        'efficientnet-b6': (1.8, 2.6, 528, 0.5),\n",
    "        'efficientnet-b7': (2.0, 3.1, 600, 0.5),\n",
    "    }\n",
    "    \n",
    "    if model_name not in params_dict.keys():\n",
    "        raise KeyError('There is no model named {}.'.format(model_name))\n",
    "\n",
    "    width_coefficient, depth_coefficient, _, dropout_rate = params_dict[model_name]\n",
    "\n",
    "    blocks_args = [\n",
    "        'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',\n",
    "        'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',\n",
    "        'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',\n",
    "        'r1_k3_s11_e6_i192_o320_se0.25',\n",
    "    ]\n",
    "    global_params = GlobalParams(\n",
    "        batch_norm_momentum=0.99,\n",
    "        batch_norm_epsilon=1e-3,\n",
    "        dropout_rate=dropout_rate,\n",
    "        drop_connect_rate=0.2,\n",
    "        num_classes=1000,\n",
    "        width_coefficient=width_coefficient,\n",
    "        depth_coefficient=depth_coefficient,\n",
    "        depth_divisor=8,\n",
    "        min_depth=None)\n",
    "\n",
    "    if override_params:\n",
    "        global_params = global_params._replace(**override_params)\n",
    "\n",
    "    decoder = BlockDecoder()\n",
    "    return decoder.decode(blocks_args), global_params\n",
    "\n",
    "\n",
    "class BlockDecoder(object):\n",
    "    \"\"\"Block Decoder for readability\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _decode_block_string(block_string):\n",
    "        \"\"\"Gets a block through a string notation of arguments.\"\"\"\n",
    "        assert isinstance(block_string, str)\n",
    "        ops = block_string.split('_')\n",
    "        options = {}\n",
    "        for op in ops:\n",
    "            splits = re.split(r'(\\d.*)', op)\n",
    "            if len(splits) >= 2:\n",
    "                key, value = splits[:2]\n",
    "                options[key] = value\n",
    "\n",
    "        if 's' not in options or len(options['s']) != 2:\n",
    "            raise ValueError('Strides options should be a pair of integers.')\n",
    "\n",
    "        return BlockArgs(\n",
    "            kernel_size=int(options['k']),\n",
    "            num_repeat=int(options['r']),\n",
    "            input_filters=int(options['i']),\n",
    "            output_filters=int(options['o']),\n",
    "            expand_ratio=int(options['e']),\n",
    "            id_skip=('noskip' not in block_string),\n",
    "            se_ratio=float(options['se']) if 'se' in options else None,\n",
    "            strides=[int(options['s'][0]), int(options['s'][1])]\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _encode_block_string(block):\n",
    "        \"\"\"Encodes a block to a string.\"\"\"\n",
    "        args = [\n",
    "            'r%d' % block.num_repeat,\n",
    "            'k%d' % block.kernel_size,\n",
    "            's%d%d' % (block.strides[0], block.strides[1]),\n",
    "            'e%s' % block.expand_ratio,\n",
    "            'i%d' % block.input_filters,\n",
    "            'o%d' % block.output_filters\n",
    "        ]\n",
    "        if 0 < block.se_ratio <= 1:\n",
    "            args.append('se%s' % block.se_ratio)\n",
    "        if block.id_skip is False:\n",
    "            args.append('noskip')\n",
    "        return '_'.join(args)\n",
    "\n",
    "    def decode(self, string_list):\n",
    "        \"\"\"Decodes a list of string notations to specify blocks inside the network.\n",
    "        Args:\n",
    "          string_list: a list of strings, each string is a notation of block.\n",
    "        Returns:\n",
    "          A list of namedtuples to represent blocks arguments.\n",
    "        \"\"\"\n",
    "        assert isinstance(string_list, list)\n",
    "        blocks_args = []\n",
    "        for block_string in string_list:\n",
    "            blocks_args.append(self._decode_block_string(block_string))\n",
    "        return blocks_args\n",
    "\n",
    "    def encode(self, blocks_args):\n",
    "        \"\"\"Encodes a list of Blocks to a list of strings.\n",
    "        Args:\n",
    "          blocks_args: A list of namedtuples to represent blocks arguments.\n",
    "        Returns:\n",
    "          a list of strings, each string is a notation of block.\n",
    "        \"\"\"\n",
    "        block_strings = []\n",
    "        for block in blocks_args:\n",
    "            block_strings.append(self._encode_block_string(block))\n",
    "        return block_strings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## efficientnet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.hub import load_state_dict_from_url\n",
    "\n",
    "class EfficientNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block_args_list, global_params):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block_args_list = block_args_list\n",
    "        self.global_params = global_params\n",
    "\n",
    "        # Batch norm parameters\n",
    "        batch_norm_momentum = 1 - self.global_params.batch_norm_momentum\n",
    "        batch_norm_epsilon = self.global_params.batch_norm_epsilon\n",
    "\n",
    "        # Stem\n",
    "        in_channels = 3\n",
    "        out_channels = round_filters(32, self.global_params)\n",
    "        self._conv_stem = Conv2dSamePadding(in_channels,\n",
    "                                            out_channels,\n",
    "                                            kernel_size=3,\n",
    "                                            stride=2,\n",
    "                                            bias=False,\n",
    "                                            name='stem_conv')\n",
    "        self._bn0 = BatchNorm2d(num_features=out_channels,\n",
    "                                momentum=batch_norm_momentum,\n",
    "                                eps=batch_norm_epsilon,\n",
    "                                name='stem_batch_norm')\n",
    "\n",
    "        self._swish = Swish(name='swish')\n",
    "\n",
    "        # Build _blocks\n",
    "        idx = 0\n",
    "        self._blocks = nn.ModuleList([])\n",
    "        for block_args in self.block_args_list:\n",
    "\n",
    "            # Update block input and output filters based on depth multiplier.\n",
    "            block_args = block_args._replace(\n",
    "                input_filters=round_filters(block_args.input_filters, self.global_params),\n",
    "                output_filters=round_filters(block_args.output_filters, self.global_params),\n",
    "                num_repeat=round_repeats(block_args.num_repeat, self.global_params)\n",
    "            )\n",
    "\n",
    "            # The first block needs to take care of stride and filter size increase.\n",
    "            self._blocks.append(MBConvBlock(block_args, self.global_params, idx=idx))\n",
    "            idx += 1\n",
    "\n",
    "            if block_args.num_repeat > 1:\n",
    "                block_args = block_args._replace(input_filters=block_args.output_filters, strides=1)\n",
    "\n",
    "            # The rest of the _blocks\n",
    "            for _ in range(block_args.num_repeat - 1):\n",
    "                self._blocks.append(MBConvBlock(block_args, self.global_params, idx=idx))\n",
    "                idx += 1\n",
    "\n",
    "        # Head\n",
    "        in_channels = block_args.output_filters  # output of final block\n",
    "        out_channels = round_filters(1280, self.global_params)\n",
    "        self._conv_head = Conv2dSamePadding(in_channels,\n",
    "                                            out_channels,\n",
    "                                            kernel_size=1,\n",
    "                                            bias=False,\n",
    "                                            name='head_conv')\n",
    "        self._bn1 = BatchNorm2d(num_features=out_channels,\n",
    "                                momentum=batch_norm_momentum,\n",
    "                                eps=batch_norm_epsilon,\n",
    "                                name='head_batch_norm')\n",
    "\n",
    "        # Final linear layer\n",
    "        self.dropout_rate = self.global_params.dropout_rate\n",
    "        self._fc = nn.Linear(out_channels, self.global_params.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Stem\n",
    "        x = self._conv_stem(x)\n",
    "        x = self._bn0(x)\n",
    "        x = self._swish(x)\n",
    "\n",
    "        # Blocks\n",
    "        for idx, block in enumerate(self._blocks):\n",
    "            drop_connect_rate = self.global_params.drop_connect_rate\n",
    "            if drop_connect_rate:\n",
    "                drop_connect_rate *= idx / len(self._blocks)\n",
    "            x = block(x, drop_connect_rate)\n",
    "\n",
    "        # Head\n",
    "        x = self._conv_head(x)\n",
    "        x = self._bn1(x)\n",
    "        x = self._swish(x)\n",
    "\n",
    "        # Pooling and Dropout\n",
    "        x = F.adaptive_avg_pool2d(x, 1).squeeze(-1).squeeze(-1)\n",
    "        if self.dropout_rate > 0:\n",
    "            x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "\n",
    "        # Fully-connected layer\n",
    "        x = self._fc(x)\n",
    "        return x\n",
    "\n",
    "    @classmethod\n",
    "    def from_name(cls, model_name, *, n_classes=1000, pretrained=False):\n",
    "        return _get_model_by_name(model_name, classes=n_classes, pretrained=pretrained)\n",
    "\n",
    "    @classmethod\n",
    "    def encoder(cls, model_name, *, pretrained=False):\n",
    "        model = cls.from_name(model_name, pretrained=pretrained)\n",
    "\n",
    "        class Encoder(nn.Module):\n",
    "            def __init__(self):\n",
    "                super().__init__()\n",
    "\n",
    "                self.name = model_name[:15]\n",
    "\n",
    "                self.global_params = model.global_params\n",
    "\n",
    "                self.stem_conv = model._conv_stem\n",
    "                self.stem_batch_norm = model._bn0\n",
    "                self.stem_swish = Swish(name='stem_swish')\n",
    "                self.blocks = model._blocks\n",
    "                self.head_conv = model._conv_head\n",
    "                self.head_batch_norm = model._bn1\n",
    "                self.head_swish = Swish(name='head_swish')\n",
    "\n",
    "            def forward(self, x):\n",
    "                # Stem\n",
    "                x = self.stem_conv(x)\n",
    "                x = self.stem_batch_norm(x)\n",
    "                x = self.stem_swish(x)\n",
    "\n",
    "                # Blocks\n",
    "                for idx, block in enumerate(self.blocks):\n",
    "                    drop_connect_rate = self.global_params.drop_connect_rate\n",
    "                    if drop_connect_rate:\n",
    "                        drop_connect_rate *= idx / len(self.blocks)\n",
    "                    x = block(x, drop_connect_rate)\n",
    "\n",
    "                # Head\n",
    "                x = self.head_conv(x)\n",
    "                x = self.head_batch_norm(x)\n",
    "                x = self.head_swish(x)\n",
    "                return x\n",
    "\n",
    "        return Encoder()\n",
    "\n",
    "    @classmethod\n",
    "    def custom_head(cls, model_name, *, n_classes=1000, pretrained=False):\n",
    "        model_name = model_name[:15]\n",
    "        if n_classes == 1000:\n",
    "            return cls.from_name(model_name, n_classes=n_classes, pretrained=pretrained)\n",
    "        else:\n",
    "            class CustomHead(nn.Module):\n",
    "                def __init__(self, out_channels):\n",
    "                    super().__init__()\n",
    "                    self.encoder = cls.encoder(model_name, pretrained=pretrained)\n",
    "                    self.custom_head = custom_head(self.n_channels * 2, out_channels)\n",
    "\n",
    "                @property\n",
    "                def n_channels(self):\n",
    "                    n_channels_dict = {'efficientnet-b0': 1280, 'efficientnet-b1': 1280, 'efficientnet-b2': 1408,\n",
    "                                       'efficientnet-b3': 1536, 'efficientnet-b4': 1792, 'efficientnet-b5': 2048,\n",
    "                                       'efficientnet-b6': 2304, 'efficientnet-b7': 2560,}\n",
    "                    return n_channels_dict[self.encoder.name]\n",
    "\n",
    "                def forward(self, x):\n",
    "                    x = self.encoder(x)\n",
    "                    mp = nn.AdaptiveMaxPool2d(output_size=(1, 1))(x)\n",
    "                    ap = nn.AdaptiveAvgPool2d(output_size=(1, 1))(x)\n",
    "                    x = torch.cat([mp, ap], dim=1)\n",
    "                    x = x.view(x.size(0), -1)\n",
    "                    x = self.custom_head(x)\n",
    "\n",
    "                    return x\n",
    "\n",
    "            return CustomHead(n_classes)\n",
    "\n",
    "\n",
    "def _get_model_by_name(model_name, classes=1000, pretrained=False):\n",
    "    block_args_list, global_params = get_efficientnet_params(model_name[:15], override_params={'num_classes': classes})\n",
    "    model = EfficientNet(block_args_list, global_params)\n",
    "    try:\n",
    "        if pretrained:\n",
    "            if len(model_name) < 16:\n",
    "                pretrained_state_dict = load_state_dict_from_url(IMAGENET_WEIGHTS[model_name[:15]])\n",
    "            else: \n",
    "                pretrained_state_dict = torch.load(IMAGENET_WEIGHTS[model_name])\n",
    "                model._fc = nn.Linear(1792, 4)\n",
    "\n",
    "            if classes != 1000:\n",
    "                random_state_dict = model.state_dict()\n",
    "                pretrained_state_dict['_fc.weight'] = random_state_dict['_fc.weight']\n",
    "                pretrained_state_dict['_fc.bias'] = random_state_dict['_fc.bias']\n",
    "\n",
    "            model.load_state_dict(pretrained_state_dict)\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"NOTE: Currently model {e} doesn't have pretrained weights, therefore a model with randomly initialized\"\n",
    "              \" weights is returned.\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## efficientunet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import os\n",
    "\n",
    "__all__ = ['EfficientUnet', 'get_efficientunet_b0', 'get_efficientunet_b1', 'get_efficientunet_b2',\n",
    "           'get_efficientunet_b3', 'get_efficientunet_b4', 'get_efficientunet_b5', 'get_efficientunet_b6',\n",
    "           'get_efficientunet_b7', 'get_socar_efficientunet_b0', 'get_socar_efficientunet_b1',\n",
    "           'get_stanford_efficientunet_b0', 'get_socar_efficientunet_b4', 'get_stanford_efficientunet_b4']\n",
    "\n",
    "\n",
    "def get_blocks_to_be_concat(model, x):\n",
    "    shapes = set()\n",
    "    blocks = OrderedDict()\n",
    "    hooks = []\n",
    "    count = 0\n",
    "\n",
    "    def register_hook(module):\n",
    "\n",
    "        def hook(module, input, output):\n",
    "            try:\n",
    "                nonlocal count\n",
    "                if module.name == f'blocks_{count}_output_batch_norm':\n",
    "                    count += 1\n",
    "                    shape = output.size()[-2:]\n",
    "                    if shape not in shapes:\n",
    "                        shapes.add(shape)\n",
    "                        blocks[module.name] = output\n",
    "\n",
    "                elif module.name == 'head_swish':\n",
    "                    # when module.name == 'head_swish', it means the program has already got all necessary blocks for\n",
    "                    # concatenation. In my dynamic unet implementation, I first upscale the output of the backbone,\n",
    "                    # (in this case it's the output of 'head_swish') concatenate it with a block which has the same\n",
    "                    # Height & Width (image size). Therefore, after upscaling, the output of 'head_swish' has bigger\n",
    "                    # image size. The last block has the same image size as 'head_swish' before upscaling. So we don't\n",
    "                    # really need the last block for concatenation. That's why I wrote `blocks.popitem()`.\n",
    "                    blocks.popitem()\n",
    "                    blocks[module.name] = output\n",
    "\n",
    "            except AttributeError:\n",
    "                pass\n",
    "\n",
    "        if (\n",
    "                not isinstance(module, nn.Sequential)\n",
    "                and not isinstance(module, nn.ModuleList)\n",
    "                and not (module == model)\n",
    "        ):\n",
    "            hooks.append(module.register_forward_hook(hook))\n",
    "\n",
    "    # register hook\n",
    "    model.apply(register_hook)\n",
    "\n",
    "    # make a forward pass to trigger the hooks\n",
    "    model(x)\n",
    "\n",
    "    # remove these hooks\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    return blocks\n",
    "\n",
    "\n",
    "class EfficientUnet(nn.Module):\n",
    "    def __init__(self, encoder, out_channels=2, concat_input=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.concat_input = concat_input\n",
    "\n",
    "        self.up_conv1 = up_conv(self.n_channels, 512)\n",
    "        self.double_conv1 = double_conv(self.size[0], 512)\n",
    "        self.up_conv2 = up_conv(512, 256)\n",
    "        self.double_conv2 = double_conv(self.size[1], 256)\n",
    "        self.up_conv3 = up_conv(256, 128)\n",
    "        self.double_conv3 = double_conv(self.size[2], 128)\n",
    "        self.up_conv4 = up_conv(128, 64)\n",
    "        self.double_conv4 = double_conv(self.size[3], 64)\n",
    "\n",
    "        if self.concat_input:\n",
    "            self.up_conv_input = up_conv(64, 32)\n",
    "            self.double_conv_input = double_conv(self.size[4], 32)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(self.size[5], out_channels, kernel_size=1)\n",
    "\n",
    "    @property\n",
    "    def n_channels(self):\n",
    "        n_channels_dict = {'efficientnet-b0': 1280, 'efficientnet-b1': 1280, 'efficientnet-b2': 1408,\n",
    "                           'efficientnet-b3': 1536, 'efficientnet-b4': 1792, 'efficientnet-b5': 2048,\n",
    "                           'efficientnet-b6': 2304, 'efficientnet-b7': 2560, }\n",
    "        return n_channels_dict[self.encoder.name]\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        size_dict = {'efficientnet-b0': [592, 296, 152, 80, 35, 32], 'efficientnet-b1': [592, 296, 152, 80, 35, 32],\n",
    "                     'efficientnet-b2': [600, 304, 152, 80, 35, 32], 'efficientnet-b3': [608, 304, 160, 88, 35, 32],\n",
    "                     'efficientnet-b4': [624, 312, 160, 88, 35, 32], 'efficientnet-b5': [640, 320, 168, 88, 35, 32],\n",
    "                     'efficientnet-b6': [656, 328, 168, 96, 35, 32], 'efficientnet-b7': [672, 336, 176, 96, 35, 32],}\n",
    "        return size_dict[self.encoder.name]\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_ = x\n",
    "\n",
    "        blocks = get_blocks_to_be_concat(self.encoder, x)\n",
    "        _, x = blocks.popitem()\n",
    "\n",
    "        x = self.up_conv1(x)\n",
    "        x = torch.cat([x, blocks.popitem()[1]], dim=1)\n",
    "        x = self.double_conv1(x)\n",
    "\n",
    "        x = self.up_conv2(x)\n",
    "        x = torch.cat([x, blocks.popitem()[1]], dim=1)\n",
    "        x = self.double_conv2(x)\n",
    "\n",
    "        x = self.up_conv3(x)\n",
    "        x = torch.cat([x, blocks.popitem()[1]], dim=1)\n",
    "        x = self.double_conv3(x)\n",
    "\n",
    "        x = self.up_conv4(x)\n",
    "        x = torch.cat([x, blocks.popitem()[1]], dim=1)\n",
    "        x = self.double_conv4(x)\n",
    "\n",
    "        if self.concat_input:\n",
    "            x = self.up_conv_input(x)\n",
    "            x = torch.cat([x, input_], dim=1)\n",
    "            x = self.double_conv_input(x)\n",
    "\n",
    "        x = self.final_conv(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def get_efficientunet_b0(out_channels=2, concat_input=True, pretrained=True):\n",
    "    encoder = EfficientNet.encoder('efficientnet-b0', pretrained=pretrained)\n",
    "    model = EfficientUnet(encoder, out_channels=out_channels, concat_input=concat_input)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_efficientunet_b1(out_channels=2, concat_input=True, pretrained=True):\n",
    "    encoder = EfficientNet.encoder('efficientnet-b1', pretrained=pretrained)\n",
    "    model = EfficientUnet(encoder, out_channels=out_channels, concat_input=concat_input)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_efficientunet_b2(out_channels=2, concat_input=True, pretrained=True):\n",
    "    encoder = EfficientNet.encoder('efficientnet-b2', pretrained=pretrained)\n",
    "    model = EfficientUnet(encoder, out_channels=out_channels, concat_input=concat_input)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_efficientunet_b3(out_channels=2, concat_input=True, pretrained=True):\n",
    "    encoder = EfficientNet.encoder('efficientnet-b3', pretrained=pretrained)\n",
    "    model = EfficientUnet(encoder, out_channels=out_channels, concat_input=concat_input)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_efficientunet_b4(out_channels=2, concat_input=True, pretrained=True):\n",
    "    encoder = EfficientNet.encoder('efficientnet-b4', pretrained=pretrained)\n",
    "    model = EfficientUnet(encoder, out_channels=out_channels, concat_input=concat_input)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_efficientunet_b5(out_channels=2, concat_input=True, pretrained=True):\n",
    "    encoder = EfficientNet.encoder('efficientnet-b5', pretrained=pretrained)\n",
    "    model = EfficientUnet(encoder, out_channels=out_channels, concat_input=concat_input)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_efficientunet_b6(out_channels=2, concat_input=True, pretrained=True):\n",
    "    encoder = EfficientNet.encoder('efficientnet-b6', pretrained=pretrained)\n",
    "    model = EfficientUnet(encoder, out_channels=out_channels, concat_input=concat_input)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_efficientunet_b7(out_channels=2, concat_input=True, pretrained=True):\n",
    "    encoder = EfficientNet.encoder('efficientnet-b7', pretrained=pretrained)\n",
    "    model = EfficientUnet(encoder, out_channels=out_channels, concat_input=concat_input)\n",
    "    return model\n",
    "\n",
    "def get_efficientunet_b0(out_channels=2, concat_input=True, pretrained=True):\n",
    "    encoder = EfficientNet.encoder('efficientnet-b0', pretrained=pretrained)\n",
    "    model = EfficientUnet(encoder, out_channels=out_channels, concat_input=concat_input)\n",
    "    return model\n",
    "\n",
    "def get_socar_efficientunet_b0(out_channels=2, concat_input=True, pretrained=True):\n",
    "    encoder = EfficientNet.encoder('efficientnet-b0-socar', pretrained=pretrained)\n",
    "    model = EfficientUnet(encoder, out_channels=out_channels, concat_input=concat_input)\n",
    "    return model\n",
    "\n",
    "def get_socar_efficientunet_b1(out_channels=2, concat_input=True, pretrained=True):\n",
    "    encoder = EfficientNet.encoder('efficientnet-b1-socar', pretrained=pretrained)\n",
    "    model = EfficientUnet(encoder, out_channels=out_channels, concat_input=concat_input)\n",
    "    return model\n",
    "\n",
    "def get_stanford_efficientunet_b0(out_channels=2, concat_input=True, pretrained=True):\n",
    "    encoder = EfficientNet.encoder('efficientnet-b0-stanford', pretrained=pretrained)\n",
    "    model = EfficientUnet(encoder, out_channels=out_channels, concat_input=concat_input)\n",
    "    return model\n",
    "\n",
    "def get_stanford_efficientunet_b4(out_channels=2, concat_input=True, pretrained=True):\n",
    "    encoder = EfficientNet.encoder('efficientnet-b4-stanford', pretrained=pretrained)\n",
    "    model = EfficientUnet(encoder, out_channels=out_channels, concat_input=concat_input)\n",
    "    return model\n",
    "\n",
    "def get_socar_efficientunet_b4(out_channels=2, concat_input=True, pretrained=True):\n",
    "    encoder = EfficientNet.encoder('efficientnet-b4-socar', pretrained=pretrained)\n",
    "    model = EfficientUnet(encoder, out_channels=out_channels, concat_input=concat_input)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "sweep_config = {\n",
    "    'method': 'grid',\n",
    "    'name':'grid-socar_sweep_v13_spacing',\n",
    "    'metric' : {\n",
    "        'name': 'Best val IoU',\n",
    "        'goal': 'maximize'   \n",
    "        },\n",
    "    'parameters' : {\n",
    "        'epochs': {\n",
    "            'value' : 30},\n",
    "        'batch_size': {\n",
    "            'value' : 16},\n",
    "        'optimizer': { \n",
    "            'value': 'adabelief'}, \n",
    "        'model': {\n",
    "            'value': 'imagenet-b1'},\n",
    "        'loss': { \n",
    "            'value': 'focal'}, \n",
    "        'img_size': { \n",
    "            'value': 512}, \n",
    "        'seed':{\n",
    "            'value': 0},\n",
    "        'learning_rate': {\n",
    "            'value': 1e-3}, \n",
    "    }\n",
    "}\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def mkdir(*paths):\n",
    "    for path in paths:\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "\n",
    "data_folder = 'scratch'\n",
    "\n",
    "ROOT_DIR = os.path.join('/home/pung/repo/', 'kimin-lab')\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'accida_masked_only_dataset_v1', data_folder)\n",
    "CKPT_DIR = os.path.join(ROOT_DIR, 'checkpoints_dir', 'checkpoints_')\n",
    "\n",
    "RESULTS_DIR = os.path.join(ROOT_DIR, 'test_results_dir', 'test_results_')\n",
    "INFER_DIR  = os.path.join(ROOT_DIR, 'inference_dir', data_folder)\n",
    "\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, 'train')\n",
    "VAL_DIR = os.path.join(DATA_DIR, 'valid')\n",
    "TEST_DIR = os.path.join(DATA_DIR, 'test')\n",
    "\n",
    "TRAIN_IMGS_DIR = os.path.join(TRAIN_DIR, 'images')\n",
    "VAL_IMGS_DIR = os.path.join(VAL_DIR, 'images')\n",
    "TEST_IMGS_DIR = os.path.join(TEST_DIR, 'images')\n",
    "\n",
    "TRAIN_LABELS_DIR = os.path.join(TRAIN_DIR, 'masks')\n",
    "VAL_LABELS_DIR = os.path.join(VAL_DIR, 'masks')\n",
    "TEST_LABELS_DIR = os.path.join(TEST_DIR, 'masks')\n",
    "\n",
    "mkdir(\n",
    "    CKPT_DIR, RESULTS_DIR, TRAIN_DIR, VAL_DIR, TEST_DIR, TRAIN_IMGS_DIR, VAL_IMGS_DIR, \n",
    "    TEST_IMGS_DIR, TRAIN_LABELS_DIR, VAL_LABELS_DIR, TEST_LABELS_DIR, \n",
    "    )\n",
    "\n",
    "# Hyper parameters\n",
    "class Config:\n",
    "    LEARNING_RATE = 1e-3\n",
    "    BATCH_SIZE = 16\n",
    "    NUM_EPOCHS = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "__all__ = ['to_numpy', 'denormalization', 'classify_class', 'save_net', 'load_net']\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    if tensor.ndim == 3:\n",
    "        return tensor.to('cpu').detach().numpy()\n",
    "    return tensor.to('cpu').detach().numpy().transpose(0, 2, 3, 1)  # (Batch, H, W, C)\n",
    "\n",
    "def denormalization(data, mean, std):\n",
    "    return (data * std) + mean\n",
    "\n",
    "def classify_class(x):\n",
    "    return 1.0 * (x > 0.5)\n",
    "\n",
    "def save_net(ckpt_dir, net, optim, epoch, is_best=False, best_iou=None):\n",
    "    if not os.path.exists(ckpt_dir):\n",
    "        os.makedirs(ckpt_dir)\n",
    "    \n",
    "    if is_best == False:\n",
    "        torch.save(\n",
    "            {'net': net.state_dict(),'optim': optim.state_dict()},\n",
    "            os.path.join(ckpt_dir, f'model_epoch_{epoch:04}.pth'),\n",
    "        )\n",
    "    elif is_best == True:\n",
    "        torch.save(\n",
    "            {'net': net.state_dict(),'optim': optim.state_dict()},\n",
    "            os.path.join(ckpt_dir, f'best_model_{best_iou:.3f}.pth'),\n",
    "        )\n",
    "\n",
    "\n",
    "def load_net(ckpt_dir, net, optim):\n",
    "    if not os.path.exists(ckpt_dir):\n",
    "        os.makedirs(ckpt_dir)\n",
    "        \n",
    "    ckpt_list = os.listdir(ckpt_dir)\n",
    "    ckpt_list.sort(key=lambda fname: int(''.join(filter(str.isdigit, fname))))\n",
    "    \n",
    "    ckpt_path = os.path.join(ckpt_dir, ckpt_list[-1])\n",
    "    model_dict = torch.load(ckpt_path)\n",
    "    print(f'* Load {ckpt_path}')\n",
    "\n",
    "    net.load_state_dict(model_dict['net'])\n",
    "    optim.load_state_dict(model_dict['optim'])\n",
    "    epoch = int(''.join(filter(str.isdigit, ckpt_list[-1])))\n",
    "    \n",
    "    return net, optim, ckpt_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import transforms\n",
    "\n",
    "class DatasetV2(Dataset):\n",
    "    def __init__(self, imgs_dir, mask_dir, transform=None):\n",
    "        self.img_dir = imgs_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.masks  = []\n",
    "\n",
    "        images_path, masks_path = [], []\n",
    "\n",
    "        for ext in ('*.jpeg', '*.png', '*.jpg'):\n",
    "            images_path.extend(sorted(glob(os.path.join(imgs_dir, ext))))\n",
    "            masks_path.extend(sorted(glob(os.path.join(mask_dir, ext))))\n",
    "        \n",
    "        for i, m  in zip(images_path, masks_path):\n",
    "            self.images.extend([Image.open(i).convert('RGB')])\n",
    "            self.masks.extend([Image.open(m).convert('L')])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.masks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        mask  = self.masks[idx]\n",
    "\n",
    "        np_image = np.array(image)\n",
    "        np_mask  = np.array(mask)\n",
    "        \n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=np_image, mask=np_mask)\n",
    "            np_image = transformed[\"image\"]\n",
    "            np_mask = transformed[\"mask\"]\n",
    "            np_mask = np_mask.long()\n",
    "        \n",
    "        ret = {\n",
    "            'img': np_image,\n",
    "            'label': np_mask,\n",
    "        }\n",
    "        \n",
    "        return ret\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    transform = A.Compose([\n",
    "                A.Resize(512, 512),\n",
    "                A.Normalize(mean=0.5, std=0.5),\n",
    "                transforms.ToTensorV2()\n",
    "            ])\n",
    "\n",
    "    img_dir = os.path.join(os.getcwd(), \"accida_segmentation\", 'imgs', 'train')\n",
    "    mask_dir = os.path.join(os.getcwd(), \"accida_segmentation\", 'labels', 'train')\n",
    "\n",
    "    train_dataset = DatasetV2(img_dir, mask_dir, transform=transform)\n",
    "    dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=1, num_workers=0)\n",
    "\n",
    "    #For shape test\n",
    "    for ret in iter(dataloader):\n",
    "        print(ret['img'].shape, ret['label'].shape, ret['label'].type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# losses.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "try:\n",
    "    from itertools import  ifilterfalse\n",
    "except ImportError: # py3k\n",
    "    from itertools import  filterfalse as ifilterfalse\n",
    "\n",
    "# --------------------------- FOCAL LOSSES ---------------------------\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=0, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha,(float,int)): self.alpha = torch.Tensor([alpha,1-alpha])\n",
    "        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim()>2:\n",
    "            input = input.view(input.size(0),input.size(1),-1)  # N,C,H,W => N,C,H*W\n",
    "            input = input.transpose(1,2)    # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1,input.size(2))   # N,H*W,C => N*H*W,C\n",
    "        target = target.view(-1,1)\n",
    "\n",
    "        logpt = F.log_softmax(input)\n",
    "        logpt = logpt.gather(1,target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = Variable(logpt.data.exp())\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type()!=input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0,target.data.view(-1))\n",
    "            logpt = logpt * Variable(at)\n",
    "\n",
    "        loss = -1 * (1-pt)**self.gamma * logpt\n",
    "        if self.size_average: return loss.mean()\n",
    "        else: return loss.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# metric.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def iou_score(output, target):\n",
    "    smooth = 1e-5\n",
    "\n",
    "    if torch.is_tensor(output):\n",
    "        output = torch.sigmoid(output).data.cpu().numpy()\n",
    "    if torch.is_tensor(target):\n",
    "        target = target.data.cpu().numpy()\n",
    "    output_ = output > 0.5\n",
    "    target_ = target > 0.5\n",
    "    intersection = (output_ & target_).sum()\n",
    "    union = (output_ | target_).sum()\n",
    "\n",
    "    return (intersection + smooth) / (union + smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sweep_eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "def eval_model(test_loader, test_batch_num, net, criterion, optim, ckpt_dir, wandb, w_config):\n",
    "    # Load Checkpoint File\n",
    "    if os.listdir(ckpt_dir):\n",
    "        net, optim, ckpt_path = load_net(ckpt_dir=ckpt_dir, net=net, optim=optim)\n",
    "\n",
    "    result_dir = os.path.join(INFER_DIR, 'test_' + ckpt_path.split('/')[-1][:-4])\n",
    "\n",
    "    if not os.path.exists(result_dir):\n",
    "        os.makedirs(result_dir)\n",
    "\n",
    "    # Evaluation\n",
    "    with torch.no_grad():\n",
    "        net.eval()  # Evaluation Mode\n",
    "        loss_arr, iou_arr = [], []\n",
    "\n",
    "        for batch_idx, data in enumerate(test_loader, 1):\n",
    "            # Forward Propagation\n",
    "            img = data['img'].to(device)\n",
    "            label = data['label'].to(device)\n",
    "\n",
    "            label = label // 255\n",
    "\n",
    "            output = net(img)\n",
    "            output_t = torch.argmax(output, dim=1).float()\n",
    "\n",
    "            # Calc Loss Function\n",
    "            loss = criterion(output, label)\n",
    "            iou = iou_score(output_t, label)\n",
    "\n",
    "            loss_arr.append(loss.item())\n",
    "            iou_arr.append(iou.item())\n",
    "            \n",
    "            print_form = '[Test] | Batch: {:0>4d} / {:0>4d} | Loss: {:.4f} | IoU: {:.4f}'\n",
    "            print(print_form.format(batch_idx, test_batch_num, loss_arr[-1], iou))\n",
    "\n",
    "            img = to_numpy(denormalization(img, mean=0.5, std=0.5))\n",
    "            # 이미지 캐스팅\n",
    "            img = np.clip(img, 0, 1) \n",
    "\n",
    "            label = to_numpy(label)\n",
    "            output_t = to_numpy(classify_class(output_t))\n",
    "            \n",
    "            for j in range(label.shape[0]):\n",
    "                crt_id = int(w_config.batch_size * (batch_idx - 1) + j)\n",
    "                \n",
    "                plt.imsave(os.path.join(result_dir, f'img_{crt_id:04}.png'), img[j].squeeze(), cmap='gray')\n",
    "                plt.imsave(os.path.join(result_dir, f'label_{crt_id:04}.png'), label[j].squeeze(), cmap='gray')\n",
    "                plt.imsave(os.path.join(result_dir, f'output_{crt_id:04}.png'), output_t[j].squeeze(), cmap='gray')\n",
    "    \n",
    "    eval_loss_avg = np.mean(loss_arr)\n",
    "    eval_iou_avg  = np.mean(iou_arr)\n",
    "    print_form = '[Result] | Avg Loss: {:0.4f} | Avg IoU: {:0.4f}'\n",
    "    wandb.log({'eval_loss': eval_loss_avg , 'eval_iou': eval_iou_avg}, commit=False)\n",
    "    print(print_form.format(eval_loss_avg, eval_iou_avg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: ycjwqqrf\n",
      "Sweep URL: https://wandb.ai/viai/%5BVIAI%5D%20EVAL/sweeps/ycjwqqrf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: iegzguqd with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: focal\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel: imagenet-b1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adabelief\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tseed: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.7 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/viai/%5BVIAI%5D%20EVAL/runs/iegzguqd\" target=\"_blank\">true-sweep-1</a></strong> to <a href=\"https://wandb.ai/viai/%5BVIAI%5D%20EVAL\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "Sweep page: <a href=\"https://wandb.ai/viai/%5BVIAI%5D%20EVAL/sweeps/ycjwqqrf\" target=\"_blank\">https://wandb.ai/viai/%5BVIAI%5D%20EVAL/sweeps/ycjwqqrf</a><br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-pytorch from version 0.0.5.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  ---------\n",
      "adabelief-pytorch=0.0.5  1e-08  False              False\n",
      ">=0.1.0 (Current 0.2.0)  1e-16  True               True\n",
      "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
      "----------------------------------------------------------  ----------------------------------------------\n",
      "Recommended eps = 1e-8                                      Recommended eps = 1e-16\n",
      "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
      "\u001b[0m\n",
      "Weight decoupling enabled in AdaBelief\n",
      "Rectification enabled in AdaBelief\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 28985... <strong style=\"color:red\">(failed 1).</strong> Press ctrl-c to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ee6492c9b6e4f86a5090b7d45d830e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "</div><div class=\"wandb-col\">\n",
       "</div></div>\n",
       "Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">true-sweep-1</strong>: <a href=\"https://wandb.ai/viai/%5BVIAI%5D%20EVAL/runs/iegzguqd\" target=\"_blank\">https://wandb.ai/viai/%5BVIAI%5D%20EVAL/runs/iegzguqd</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211122_163743-iegzguqd/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run iegzguqd errored: FileNotFoundError(2, 'No such file or directory')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run iegzguqd errored: FileNotFoundError(2, 'No such file or directory')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Exiting.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import transforms\n",
    "from adabelief_pytorch import AdaBelief\n",
    "\n",
    "import wandb\n",
    "\n",
    "def wandb_setting(sweep_config=None):\n",
    "    wandb.init(config=sweep_config)\n",
    "    w_config = wandb.config\n",
    "    name_str = str(w_config.model) + ' | ' +  str(w_config.img_size) + ' | ' +  str(w_config.batch_size) \n",
    "    wandb.run.name = name_str\n",
    "\n",
    "    #########Random seed 고정해주기###########\n",
    "    random_seed = w_config.seed\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed) \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    ###########################################\n",
    "\n",
    "    test_transform = A.Compose([\n",
    "        A.Resize(w_config.img_size, w_config.img_size),\n",
    "        A.Normalize(mean=(0.485), std=(0.229)),\n",
    "        transforms.ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    ##########################################데이터 로드 하기#################################################\n",
    "    batch_size= w_config.batch_size\n",
    "\n",
    "    test_dataset = DatasetV2(imgs_dir=TEST_IMGS_DIR, mask_dir=TEST_LABELS_DIR, transform=test_transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    #############################################################################################################\n",
    "\n",
    "    test_data_num = len(test_dataset)\n",
    "    test_batch_num = int(np.ceil(test_data_num / batch_size)) \n",
    "\n",
    "    if w_config.model == 'imagenet-b1':\n",
    "        net = get_efficientunet_b1(out_channels=2, concat_input=True, pretrained=True).to(device)\n",
    "    elif w_config.model == 'stfd-ssl-b4':\n",
    "        net = get_stanford_efficientunet_b4(out_channels=2, concat_input=True, pretrained=True).to(device)\n",
    "    elif w_config.model == 'socar-ssl-b4':\n",
    "        net = get_socar_efficientunet_b4(out_channels=2, concat_input=True, pretrained=True).to(device)\n",
    "    \n",
    "    # Loss Function\n",
    "    if w_config.loss == 'CrossEntropy':\n",
    "        criterion = nn.CrossEntropyLoss().to(device)\n",
    "    elif w_config.loss == 'focal':\n",
    "        criterion = FocalLoss(gamma=2, alpha=0.5).to(device)\n",
    "\n",
    "    # Optimizer\n",
    "    if w_config.optimizer == 'sgd':\n",
    "        optimizer_ft = torch.optim.SGD(net.parameters(), lr=w_config.learning_rate, momentum=0.9)# optimizer 종류 정해주기\n",
    "    elif w_config.optimizer == 'adam':\n",
    "        optimizer_ft = torch.optim.Adam(params=net.parameters(), lr=w_config.learning_rate)\n",
    "    elif w_config.optimizer == 'adabelief':\n",
    "        optimizer_ft = AdaBelief(net.parameters(), lr=w_config.learning_rate, eps=1e-16, betas=(0.9,0.999), weight_decouple = True, rectify = True)\n",
    "    \n",
    "    ckpt_dir = CKPT_DIR + name_str\n",
    "\n",
    "    wandb.watch(net, log='all') \n",
    "\n",
    "    eval_model(test_loader, test_batch_num, net, criterion, optimizer_ft, ckpt_dir, wandb, w_config=w_config)\n",
    "\n",
    "project_name = '[VIAI] EVAL' # 프로젝트 이름을 설정해주세요.\n",
    "entity_name  = 'viai' # 사용자의 이름을 설정해주세요.\n",
    "sweep_id = wandb.sweep(sweep_config, project=project_name, entity=entity_name)\n",
    "\n",
    "wandb.agent(sweep_id, wandb_setting, count=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53a714998b4cda886d88c1f35ca09ebc6db63d3c7248d837ab3cd117369573cd"
  },
  "kernelspec": {
   "display_name": "Python 3.6.5 64-bit ('pytorch_p36': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
